---
title: Troubleshooting Slow Requests in Cloud Foundry
owner: Routing
---

<% current_page.data.title = "Troubleshooting Slow Requests in " + vars.app_runtime_abbr %>

This topic suggests ways that an operator of <%= vars.app_runtime_full %> (<%= vars.app_runtime_abbr %>) can diagnose the location of app request delays.


## <a id="request-path"></a> App Request Path

App requests typically transit the following components. Only the router (Gorouter) and app are within the scope of <%= vars.app_runtime_abbr %>. Operators may have the HAProxy load balancer that comes with <%= vars.app_runtime_abbr %> deployed, instead of or in addition to, an infrastructure load balancer.

<%= image_tag("request_lifecycle.png") %>

Latency can come from anyone of these components or from the network itself. This doc will provide steps to help you determine what part of this request flow is adding latency to your requests. The following experiments are intended to be run in order.


## <a id="total-latency"></a> Experiment 1: Measure Total Round-Trip App Requests

**Steps**
1. On a command line, run `time curl -v APP-ENDPOINT` to measure the total round-trip time for deployed your app that is experiencing latency. For example:

<pre class="terminal">
$ time curl -v http://app1.app_domain.com

GET /hello HTTP/1.1
Host: app1.app_domain.com
User-Agent: curl/7.43.0
Accept: */*

HTTP/1.1 200 OK
Content-Type: application/json;charset=utf-8
Date: Tue, 14 Dec 2016 00:31:32 GMT
Server: nginx
X-Content-Type-Options: nosniff
X-Vcap-Request-Id: c30fad28-4972-46eb-7da6-9d07dc79b109
Content-Length: 602
hello world!

real	2m0.707s
user	0m0.005s
sys	  0m0.007s
</pre>

The `real` time output shows that the request to `http://app1.app_domain.com` took approximately 2 minutes, round-trip. This seems like an unreasonably long time, so it makes sense to find out where the delay is occurring.

**Result: You saw latency**

Please continue with the rest of the experiments to try to narrow down which component is having problems. 

**Result: You didn't see latency**

This indicates that you are inconsistently experiencing latency. Try to find an endpoint that is consistently experiencing latency. If that is not possible, then continue with the following experiments, but run each of them multiple times to try and "catch" the latency.

**Result: You got an error**

If your <code>curl</code> outputs an error like <code>Could not resolve host: NONEXISTENT.com</code> then DNS failed to resolve. If <code>curl</code> returns normally but lacks a <code>X-Vcap-Request-Id</code>, the request from the Load Balancer did not reach <%= vars.app_runtime_abbr %>.


## <a id="within-cf"></a> Experiment 2: Look at Request Time in Access Logs

If you suspect that you are experiencing latency, the most important logs are the access logs. The `cf logs` command streams log messages from the Gorouter as well as from apps. These steps will show you how to see and understand the access log timestamps.

**Steps**
1. If necessary, run `cf apps` to determine the name of the app.
1. Run `cf logs APP-NAME`. Replace `APP-NAME` with the name of the app.
1. From another terminal window, send a request to your app.
1. After your app returns a response, enter `Ctrl-C` to stop streaming `cf logs`.

For example:

<pre class="terminal">
$ cf logs app1

2016-12-14T00:33:32.35-0800 [RTR/0] OUT app1.app_domain.com - [14/12/2016:00:31:32.348 +0000] "GET /hello HTTP/1.1" 200 0 60 "-" "HTTPClient/1.0 (2.7.1, ruby 2.3.3 (2016-11-21))" "10.0.4.207:20810" "10.0.48.67:61555" x_forwarded_for:"52.3.107.171" x_forwarded_proto:"http" vcap_request_id:"01144146-1e7a-4c77-77ab-49ae3e286fe9" response_time:120.00641734 app_id:"13ee085e-bdf5-4a48-aaaf-e854a8a975df" app_index:"0" x_b3_traceid:"3595985e7c34536a" x_b3_spanid:"3595985e7c34536a" x_b3_parentspanid:"-"
2016-12-14T00:32:32.35-0800 [APP/PROC/WEB/0]OUT app1 received request at [14/12/2016:00:32:32.348 +0000] with "vcap_request_id": "01144146-1e7a-4c77-77ab-49ae3e286fe9"
^C
</pre>

In the example above, the first line contains timestamps from the Gorouter for both when it received the request and what was its response time processing the request:

* `14/12/2016:00:31:32.348`: Gorouter receives request
* `response_time:120.00641734`: Gorouter round-trip processing time

This output shows that it took 120 seconds for the Gorouter to process the request, which means that the 2-minute delay above takes place within <%= vars.app_runtime_abbr %>, either within the Gorouter, within the app, or within the network between the two.

**Result: You saw latency within `response_time`**

If you saw latency within `response_time`, this indicates that the latency is coming from either the Gorouter, the network, or from the app itself. Please continue with the rest of the experiments to try to narrow down which component is having problems. 

**Result: You didn't see latency within `response_time`**

If you did not see latency, this indicates that the slowness is coming from a component before Gorouter. Please continue with the rest of the experiments to try to narrow down which component is having problems. 

**Result: You didn't see a log line**

Every incoming request should generate an access log message. If a request does not generate an access log message, it means the Gorouter did not receive the request.

## <a id="duplicate-latency"></a> Experiment 3: Duplicate latency with a simple endpoint

The next step to debugging latency is finding an endpoint that _consistently_ experiences latency. The best option is to use a test app, like dora, because it is a small, simple app that does not make any  external or internal requests or database calls. If you cannot push any apps to your foundation, find a simple API endpoint (like a health or info endpoint) that does not make any external calls to use for the rest of the experiments.

**Steps**
1. Push an example app, like [dora](https://github.com/cloudfoundry/cf-acceptance-tests/tree/master/assets/dora).
2. Use `time` to measure a request's full round trip time from the client and back. On a command line, run `time curl -v TEST-APP-ENDPOINT`. Every network is different, but this request should take less than 0.2 seconds.

**Result: You saw latency with the test app**

If you saw latency with the test app, this indicates that there is something going wrong with one of the components in app request path. Please continue with the rest of the experiments to try to narrow down which component is having problems. Continue using the test app for the following experiments.


**Result: You did not see latency with the test app**

If you did not see latency with the test app, this indicates that there is something different about your app that is causing latency. Evaluate your app and what has changed recently. Did you recently push any changes? Does it make database calls? Have your database queries changed? Does it make requests to other microservices? Maybe there is a problem in a downstream app? Try add logging to your app to measure where it is spending time. For more information, see [Use App Logs to Locate Delays in <%= vars.app_runtime_abbr %>](#app_logs).


## <a id="remove-the-load-balancer"></a> Experiment 4: Remove the Load Balancer from the Request Path

The next step is to remove the load balancer from the equation by sending the request directly to the Gorouter. You can do this by getting onto the network where the Gorouter is deployed, sending the traffic directly to the Gorouter IP, and adding the route in the host header.


**Steps**
1. Choose a router VM from your deployment and get its IP. Let's call this value `ROUTER-IP`.
2. Bosh ssh onto the router VM, `bosh ssh router/<ROUTER-GUID>`. For more information, see [SSH](https://bosh.io/docs/cli-v2.html#ssh-mgmt) in the BOSH documentation.
3. Time the amount of time a request takes when it skips the load balancer, `time curl ROUTER-IP -H "Host: TEST-APP-ENDPOINT"`


**Result: You saw latency**

If you saw latency with this request, this indicates that the latency is not coming from the load balancer. Please continue with the rest of the experiments to try to narrow down which component is having problems.


**Result: You did not see latency**

If you did not see latency with this request, this indicates that the latency is coming from a component before Gorouter. Look at your load balancer logs and logs for any other components that exist between the end client and the Gorouter.


## <a id="remove-the-gorouter"></a> Experiment 5: Remove the Gorouter from the Request Path

The next step is to remove the Gorouter from the equation. You can do this by sshing onto the router VM and sending a request directly to the app.

**Steps**
1. Get the Diego Cell IP and port where your test app instance is running, `cf ssh TEST-APP -c "env | grep CF_INSTANCE_ADDR"`.
2. Choose any router VM from your deployment and ssh onto it, `bosh ssh router/<ROUTER-GUID>`. 
3. Time the amount of time a request takes when it skips the Gorouter, `time curl CF_INSTANCE_ADDR`

**Result: You saw latency**

If you saw latency with this request, this indicates that the latency is not coming from the Gorouter. Please continue with the rest of the experiments to try to narrow down which component is having problems.


**Result: You did not see latency**

If you did not see latency with this request, this indicates that the latency is coming from the Gorouter (assuming you have ruled out the other components with the other experiments). Please go to the sections on [causes of Gorouter latency](gorouter-latency) and [recommendations for solving Gorouter latency problems](ops-recommendations).


## <a id="test-internal-network"></a> Experiment 6: Test the Network between the Router and the App

The next step is to time how long it takes for your request to make it from the Router VM to the Diego Cell VM where your app is deployed. You can do this by using tcpdump on both VMs.

**Steps**
1. Choose a router VM from your deployment and grab it's IP. Let's call this `ROUTER-IP`.
2. Get the Diego Cell IP where your test app instance is running, `cf ssh TEST-APP -c "env | grep CF_INSTANCE_IP"`.
3. Get the Diego Cell port where your test app instance is running, `cf ssh TEST-APP -c "env | grep CF_INSTANCE_PORT"`.
4. In one terminal, bosh ssh onto the Router VM corresponding to the ROUTER-IP, `bosh ssh router/<ROUTER-GUID>`.
5. On the Router, become root and run the following tcpdump command to capture all packets going to your app, `tcpdump 'dst CF_INSTANCE_IP and dst port CF_INSTANCE_PORT'`.
6. In a second terminal, bosh ssh onto the Diego Cell corresponding with CF_INSTANCE_IP, `bosh ssh digeo-cell/<DIEGO-CELL-GUID>`.
7. On the Diego Cell, become root and run the following tcpdump command to capture all packets going to your app, `tcpdump 'dst port CF_INSTANCE_PORT and src ROUTER-IP'`
8. In a third terminal, ssh onto the Router VM corresponding to the ROUTER-IP and curl your app, `curl CF_INSTANCE_IP:CF_INSTANCE_PORT`
9. Look at the first packet captured on the Router and Diego Cell. They should match. Use the timestamps to determine how long it took the packet to traverse the network.

Note: if you are using a test app this should be the only traffic to your app. If you are not using a test app and there is traffic to your app, then these tcpdump commands could result in many packet captures. If the tcpdump results are too verbose to track easily, write them to a pcap file and use wireshark to find the important packets. You can write tcpdump commands to a file by using the `-w` flag, for example: `tcpdump -w router.pcap`.

**Result: You saw latency**

If you saw latency, this indicates that your network is the source of the latency.

**Result: You did not see latency**

By this point, if you have done all of the experiments, you should have been able to narrow down where the latency is coming from. Make sure you have done all of these experiments in order with a test app. Go back to experiment 1 and make sure that you are consistently experiencing latency.


## <a id="app_logs"></a> Use App Logs to Locate Delays in <%= vars.app_runtime_abbr %>

To gain a more detailed picture of where delays exist in your request path, augment the logging that your app generates. For example, call your logging library from the request handler to generate log lines when your app receives a request and finishes processing it:

<pre class="terminal">
2016-12-14T00:33:32.35-0800 [RTR/0] OUT app1.app_domain.com - [14/12/2016:00:31:32.348 +0000] "GET /hello HTTP/1.1" 200 0 60 "-" "HTTPClient/1.0 (2.7.1, ruby 2.3.3 (2016-11-21))" "10.0.4.207:20810" "10.0.48.67:61555" x_forwarded_for:"52.3.107.171" x_forwarded_proto:"http" vcap_request_id:"01144146-1e7a-4c77-77ab-49ae3e286fe9" response_time:120.00641734 app_id:"13ee085e-bdf5-4a48-aaaf-e854a8a975df" app_index:"0" x_b3_traceid:"3595985e7c34536a" x_b3_spanid:"3595985e7c34536a" x_b3_parentspanid:"-"
2016-12-14T00:32:32.35-0800 [APP/PROC/WEB/0]OUT app1 received request at [14/12/2016:00:32:32.348 +0000] with "vcap_request_id": "01144146-1e7a-4c77-77ab-49ae3e286fe9"
2016-12-14T00:32:32.50-0800 [APP/PROC/WEB/0]OUT app1 finished processing req at [14/12/2016:00:32:32.500 +0000] with "vcap_request_id": "01144146-1e7a-4c77-77ab-49ae3e286fe9"
</pre>

Comparing the router access log messages from [Measure App Requests within <%= vars.app_runtime_abbr %>](#within-cf) with the new app logs above, we can construct the following timeline:

* `14/12/2016:00:31:32.348`: Gorouter receives request
* `2016-12-14T00:32:32.35`: App receives request
* `2016-12-14T00:32:32.50`: App finishes processing request
* `2016-12-14T00:33:32.35`: Gorouter finishes processing request

The timeline indicates that the Gorouter took close to 60 seconds to send the request to the app and another 60 seconds to receive the response from the app. This suggests a delay either with the Gorouter, or in network latency between the Gorouter and Diego Cells hosting the app.


## <a id="gorouter-latency"></a> Potential Causes for Gorouter Latency

Two potential causes for Gorouter latency are:

* Routers are under heavy load from incoming client requests.

* Apps are taking a long time to process requests. This increases the number of concurrent threads held open by the Gorouter, reducing capacity to handle requests for other apps.


## <a id="ops-recommendations"></a> Operations Recommendations

* Monitor CPU load for Gorouters. At high CPU (70%+), latency increases. If the Gorouter CPU reaches this threshold, consider adding another Gorouter instance.

* Monitor latency of all routers using metrics from the Firehose. Do not monitor the average latency across all routers. Instead, monitor them individually on the same graph.

* Consider using Pingdom against an app on your <%= vars.app_runtime_abbr %> deployment to monitor latency and uptime. For more information, see the [Pingdom](http://www.pingdom.com) website.

* Consider enabling access logs on your load balancer. To enable access logs, see your load balancer documentation. Just as you use Gorouter access log messages above to determine latency from the Gorouter, you can compare load balancer logs to identify latency between the load balancer and the Gorouter. You can also compare load balancer response times with the client response times to identify latency between client and load balancer.

* Deploy a nozzle to the Loggregator Firehose to track metrics for the Gorouter. For more information, see [Deploying a Nozzle to the Loggregator Firehose](../loggregator/nozzle-tutorial.html) Available metrics include:
	* CPU utilization
	* Latency
	* Requests per second
